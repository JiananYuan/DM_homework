{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253325"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tsfresh as tsf\n",
    "import random\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "df = pd.read_csv(r'C:\\\\Users\\\\Jiananyuan\\Desktop\\\\QuickAccess\\\\DM_homework\\dataset\\\\train.csv')\n",
    "signals = df['heartbeat_signals'].str.split(',', expand=True)\n",
    "signals.insert(0, '', df['label'], allow_duplicates=False)\n",
    "col_names = ['label']\n",
    "for i in range(0, 205):\n",
    "    col_names.append('signal_' + str(i))\n",
    "signals.columns = col_names\n",
    "signals = pd.DataFrame(signals, dtype=np.float64)\n",
    "\n",
    "def signal_enhance(_signal, sigma=0.1):\n",
    "    scalingFactor = np.random.normal(loc=1.0, scale=sigma, size=(1, _signal.shape[1]))\n",
    "    noise = np.matmul(np.ones((_signal.shape[0], 1)), scalingFactor)\n",
    "    return _signal * noise\n",
    "\n",
    "idx_set = signals.query('label==1').index\n",
    "record_label_1 = signals.iloc[idx_set, :].reset_index(drop=True)\n",
    "enhanced_signal_label_1 = record_label_1\n",
    "for i in np.arange(16):\n",
    "    tmp_enhanced_signal_label_1 = signal_enhance(record_label_1)\n",
    "    enhanced_signal_label_1 = pd.concat((enhanced_signal_label_1, \n",
    "                                         tmp_enhanced_signal_label_1), axis=0).reset_index(drop=True)\n",
    "enhanced_signal_label_1['label'] = 1\n",
    "\n",
    "idx_set = signals.query('label==2').index\n",
    "record_label_2 = signals.iloc[idx_set, :].reset_index(drop=True)\n",
    "enhanced_signal_label_2 = record_label_2\n",
    "for i in np.arange(3):\n",
    "    tmp_enhanced_signal_label_2 = signal_enhance(record_label_2)\n",
    "    enhanced_signal_label_2 = pd.concat((enhanced_signal_label_2, tmp_enhanced_signal_label_2), axis=0).reset_index(\n",
    "        drop=True)\n",
    "enhanced_signal_label_2['label'] = 2\n",
    "\n",
    "idx_set = signals.query('label==3').index\n",
    "record_label_3 = signals.iloc[idx_set, :].reset_index(drop=True)\n",
    "enhanced_signal_label_3 = record_label_3\n",
    "for i in np.arange(3):\n",
    "    tmp_enhanced_signal_label_3 = signal_enhance(record_label_3)\n",
    "    enhanced_signal_label_3 = pd.concat((enhanced_signal_label_3, tmp_enhanced_signal_label_3), axis=0).reset_index(\n",
    "        drop=True)\n",
    "enhanced_signal_label_3['label'] = 3\n",
    "\n",
    "idx_set = signals.query('label==0').index\n",
    "record_label_0 = signals.iloc[idx_set, :].reset_index(drop=True)\n",
    "\n",
    "data_train = pd.concat([record_label_0, \n",
    "                        enhanced_signal_label_1, \n",
    "                        enhanced_signal_label_2, \n",
    "                        enhanced_signal_label_3], ignore_index=True)\n",
    "\n",
    "def remove_last_zero(_series):\n",
    "    nps = _series.to_numpy()\n",
    "    zero_begin_idxs = np.arange(nps.shape[0])\n",
    "    for i in np.arange(nps.shape[1])[::-1]:\n",
    "        idxs = np.where(nps[zero_begin_idxs, i] <= 1.e-5)[0]\n",
    "        if idxs.size > 0:\n",
    "            nps[zero_begin_idxs[idxs], i] = np.nan\n",
    "            zero_begin_idxs = zero_begin_idxs[idxs]\n",
    "        else:\n",
    "            break\n",
    "    return pd.DataFrame(nps[:, :], index=_series.index, columns=_series.columns[:])\n",
    "\n",
    "# data_train = remove_last_zero(data_train)\n",
    "\n",
    "# drop_idx = list(data_train.index)\n",
    "# drop_idx = random.sample(drop_idx, 3325)\n",
    "# data_train = data_train.drop(drop_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "data_train = shuffle(data_train)\n",
    "net_train_data = data_train.iloc[:, 1:].to_numpy()\n",
    "net_train_label = data_train.iloc[:, 0].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 205) (200000,) (53325, 205) (53325,)\n"
     ]
    }
   ],
   "source": [
    "train_set = data_train.iloc[:200000, :]\n",
    "test_set = data_train.iloc[200000:, :]\n",
    "\n",
    "train_x = train_set.iloc[:, 1:].to_numpy()\n",
    "train_y = train_set.iloc[:, 0].to_numpy()\n",
    "valid_x = test_set.iloc[:, 1:].to_numpy()\n",
    "valid_y = test_set.iloc[:, 0].to_numpy()\n",
    "\n",
    "# train_data = data_train.iloc[:, 1:]\n",
    "# train_label = data_train.iloc[:, 0]\n",
    "\n",
    "# train_data = data_train.iloc[:, 1:].stack()\n",
    "# train_data = train_data.reset_index()\n",
    "# train_data.rename(columns={\"level_0\": \"id\", \"level_1\": \"time\", 0: \"signals\"}, \n",
    "#                   inplace=True)\n",
    "# train_data[\"signals\"] = train_data[\"signals\"].astype(float)\n",
    "\n",
    "# train_data\n",
    "\n",
    "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9912298 , 0.94353304, 0.7646773 , ...,        nan,        nan,\n",
       "               nan],\n",
       "       [0.9714822 , 0.92896875, 0.57293281, ...,        nan,        nan,\n",
       "               nan],\n",
       "       [0.97579528, 0.93408847, 0.65963666, ...,        nan,        nan,\n",
       "               nan],\n",
       "       ...,\n",
       "       [0.88023307, 0.89468516, 0.89448266, ...,        nan,        nan,\n",
       "               nan],\n",
       "       [0.71712964, 0.68011328, 0.63003724, ...,        nan,        nan,\n",
       "               nan],\n",
       "       [0.84084461, 0.5637697 , 0.63639073, ...,        nan,        nan,\n",
       "               nan]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_label.values.tolist()\n",
    "# net_train_data = train_data.values\n",
    "# net_train_label = train_label.values\n",
    "# net_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=1,\n",
    "                out_channels=64,\n",
    "                kernel_size=7,\n",
    "                stride=1,\n",
    "                padding=3,\n",
    "            ),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(64, 128, 5, 1, 2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, 3, 1, 1), \n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(6400, 4096),\n",
    "            nn.BatchNorm1d(4096),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(4096, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         print(x.size())\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import copy\n",
    "\n",
    "# 训练网络\n",
    "def train_net(net, train_loader, valid_loader, EPOCH=20, LR=1e-5):\n",
    "    # 交叉熵作为损失函数\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Adam 优化器\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "    \n",
    "    # 保留准确率最高的模型\n",
    "    best_state = copy.deepcopy(net.state_dict())\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(EPOCH):\n",
    "        net.train()\n",
    "        train_l = 0\n",
    "        train_num = 0\n",
    "        for b_x, b_y in train_loader:\n",
    "            output = net(b_x)\n",
    "            loss = loss_fn(output, b_y)\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_l += loss.cpu().item()\n",
    "            train_num += len(b_x)\n",
    "        \n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_l = 0\n",
    "            valid_num = 0\n",
    "            right_num = 0.\n",
    "            for b_x, b_y in valid_loader:\n",
    "                output = net(b_x)\n",
    "                loss = loss_fn(output, b_y)\n",
    "                valid_l += loss.cpu().item()\n",
    "\n",
    "                pred_y = torch.argmax(output, dim=1)\n",
    "                valid_num += len(b_x)\n",
    "                right_num += (pred_y == b_y).sum().item()\n",
    "        valid_acc = right_num / valid_num\n",
    "        if valid_acc >= best_acc:\n",
    "            best_acc = valid_acc\n",
    "            best_state = copy.deepcopy(net.state_dict())\n",
    "        print('Epoch {} | train_loss {:.2e} | valid_loss {:.2e} | acc {:f} | best_acc {:f}'.format(epoch + 1, train_l / train_num, valid_l / valid_num, valid_acc, best_acc))\n",
    "        \n",
    "    \n",
    "    return best_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in c:\\users\\jiananyuan\\appdata\\roaming\\python\\python38\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\jiananyuan\\appdata\\roaming\\python\\python38\\site-packages (from imblearn) (0.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\jiananyuan\\appdata\\roaming\\python\\python38\\site-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\jiananyuan\\appdata\\roaming\\python\\python38\\site-packages (from imbalanced-learn->imblearn) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\jiananyuan\\appdata\\roaming\\python\\python38\\site-packages (from imbalanced-learn->imblearn) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\jiananyuan\\appdata\\roaming\\python\\python38\\site-packages (from imbalanced-learn->imblearn) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([257932, 1, 205]) torch.Size([257932]) torch.Size([25333, 1, 205]) torch.Size([25333])\n",
      "torch.Size([257932, 1, 205]) torch.Size([257932]) torch.Size([25333, 1, 205]) torch.Size([25333])\n",
      "torch.Size([257932, 1, 205]) torch.Size([257932]) torch.Size([25333, 1, 205]) torch.Size([25333])\n",
      "torch.Size([257932, 1, 205]) torch.Size([257932]) torch.Size([25333, 1, 205]) torch.Size([25333])\n",
      "torch.Size([257932, 1, 205]) torch.Size([257932]) torch.Size([25333, 1, 205]) torch.Size([25333])\n",
      "torch.Size([257936, 1, 205]) torch.Size([257936]) torch.Size([25332, 1, 205]) torch.Size([25332])\n",
      "torch.Size([257936, 1, 205]) torch.Size([257936]) torch.Size([25332, 1, 205]) torch.Size([25332])\n",
      "torch.Size([257932, 1, 205]) torch.Size([257932]) torch.Size([25332, 1, 205]) torch.Size([25332])\n",
      "torch.Size([257932, 1, 205]) torch.Size([257932]) torch.Size([25332, 1, 205]) torch.Size([25332])\n",
      "torch.Size([257932, 1, 205]) torch.Size([257932]) torch.Size([25332, 1, 205]) torch.Size([25332])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from torch.utils.data import TensorDataset\n",
    "import time\n",
    "net_states = []\n",
    "seed = 2023\n",
    "\n",
    "# 采用 10 折交叉验证，训练 10 个模型\n",
    "sk = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "for i, (train, valid) in enumerate(sk.split(net_train_data, net_train_label)):\n",
    "    x_train = net_train_data[train]\n",
    "    y_train = net_train_label[train]\n",
    "    x_valid = net_train_data[valid]\n",
    "    y_valid = net_train_label[valid]\n",
    "    \n",
    "    # # 对训练数据进行随机过采样\n",
    "    x_train, y_train = RandomOverSampler(random_state=seed).fit_resample(x_train, y_train)\n",
    "\n",
    "    x_train = torch.tensor(x_train, dtype=torch.float).unsqueeze(-2)\n",
    "    x_valid = torch.tensor(x_valid, dtype=torch.float).unsqueeze(-2)\n",
    "    y_train = torch.tensor(y_train, dtype=int)\n",
    "    y_valid = torch.tensor(y_valid, dtype=int)\n",
    "    \n",
    "    # train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=512, shuffle=True, num_workers=2)\n",
    "    # valid_loader = DataLoader(TensorDataset(x_valid, y_valid), batch_size=1024, shuffle=False)\n",
    "    \n",
    "    # print('\\nTraining {}th net. {}\\n'.format(i + 1, time.strftime('%H:%M:%S')))\n",
    "    # net = Net()\n",
    "    # s = train_net(net, train_loader, valid_loader, 30)\n",
    "    # net_states.append(s)\n",
    "    print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape)\n",
    "\n",
    "# 保存模型参数\n",
    "torch.save(net_states, 'net_state.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200000, 1, 205]) torch.Size([200000]) torch.Size([53325, 1, 205]) torch.Size([53325])\n",
      "\n",
      "Training 10th net. 13:22:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 2023\n",
    "\n",
    "# x_train, x_label = RandomOverSampler(random_state=seed).fit_resample(x_train, x_label)\n",
    "\n",
    "train_x = torch.tensor(train_x, dtype=torch.float).unsqueeze(-2)\n",
    "valid_x = torch.tensor(valid_x, dtype=torch.float).unsqueeze(-2)\n",
    "train_y = torch.tensor(train_y, dtype=int)\n",
    "valid_y = torch.tensor(valid_y, dtype=int)\n",
    "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=512, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(TensorDataset(valid_x, valid_y), batch_size=1024, shuffle=False)\n",
    "\n",
    "print('\\nTraining {}th net. {}\\n'.format(i + 1, time.strftime('%H:%M:%S')))\n",
    "net = Net()\n",
    "s = train_net(net, train_loader, valid_loader, 30)\n",
    "torch.save(s, 'net_state.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)\n",
    "\n",
    "# 读入模型参数\n",
    "net_states = torch.load('net_state.pkl')\n",
    "for s in net_states:\n",
    "    net.load_state_dict(s)\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        cur_pred = []\n",
    "        for i in range(100):\n",
    "            b_x = test_x[i * 200: (i + 1) * 200].to(device)\n",
    "            output = nn.functional.softmax(net(b_x), dim=1).cpu().tolist()\n",
    "            cur_pred.extend(output)\n",
    "    test_pred.append(cur_pred)\n",
    "test_pred = np.array(test_pred)\n",
    "test_avg = test_pred.mean(axis=0)\n",
    "test_y_pred = test_avg.argmax(axis=1)\n",
    "test_y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
